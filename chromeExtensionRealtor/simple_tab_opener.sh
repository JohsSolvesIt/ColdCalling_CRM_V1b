#!/bin/bash

# Simple Tab Opener for Chrome Extension Batch Processing
# Opens URLs from CSV in new tabs and lets the Chrome Extension handle extraction
# No browser automation - just opens tabs naturally

set -euo pipefail

# Signal handlers for immediate stop
cleanup_and_exit() {
    echo
    print_essential "ğŸ›‘ PROCESS STOPPED BY USER"
    print_essential "Cleaning up and exiting..."
    
    # Kill any background processes
    jobs -p | xargs -r kill -TERM 2>/dev/null || true
    
    # Remove any temporary files
    rm -f temp/urls_to_process.txt 2>/dev/null || true
    
    exit 0
}

# Set up signal traps for immediate response
trap cleanup_and_exit SIGTERM SIGINT

# Configuration
readonly SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
readonly DEFAULT_CSV_FILE="90028 realtors LINKS ONLY (copy).csv"
readonly PROGRESS_FILE="simple_batch_progress.json"
readonly LOG_FILE="logs/simple_batch_$(date +%Y%m%d_%H%M%S).log"
readonly BACKEND_PORT=5001

# Processing settings (configurable via command line)
BATCH_SIZE=3           # Process multiple URLs concurrently for speed  
TAB_DELAY=1           # Seconds between processing each URL (faster processing)
PROCESSING_WAIT=300   # Max seconds to wait for extension processing (5 minutes - extension handles its own timeouts)
readonly POLLING_INTERVAL=2    # Seconds between polling checks (balanced speed/log size)
FORCE_REPROCESS=false # Skip duplicate checking when true

# Verbosity control
VERBOSE_MODE=false    # Set to true for detailed output
QUIET_MODE=true       # Set to false for normal output
SILENT_MODE=true      # Ultra-quiet mode for batch processing

# Colors
readonly RED='\033[0;31m'
readonly GREEN='\033[0;32m'
readonly YELLOW='\033[1;33m'
readonly BLUE='\033[0;34m'
readonly NC='\033[0m'

# Global variables
declare -A SUCCESSFULLY_PROCESSED_URLS  # Track URLs processed in this session
TOTAL_URLS=0
PROCESSED_URLS=0
SUCCESSFUL_EXTRACTIONS=0
SKIPPED_URLS=0
FAILED_URLS=0
LAST_REALTOR_NAME=""
LAST_REALTOR_URL=""

# ============================================================================
# LOGGING FUNCTIONS
# ============================================================================

log() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $*" >> "$LOG_FILE"
}

print_info() {
    if [[ "$VERBOSE_MODE" == "true" ]] || [[ "$QUIET_MODE" == "false" ]]; then
        echo -e "${BLUE}[INFO]${NC} $*"
    fi
    log "INFO: $*"
}

print_success() {
    if [[ "$QUIET_MODE" == "false" ]] && [[ "$SILENT_MODE" == "false" ]]; then
        echo -e "${GREEN}[SUCCESS]${NC} $*"
    fi
    log "SUCCESS: $*"
}

print_success_always() {
    # For critical success messages that should always show
    echo -e "${GREEN}[SUCCESS]${NC} $*"
    log "SUCCESS: $*"
}

print_warning() {
    if [[ "$QUIET_MODE" == "false" ]]; then
        echo -e "${YELLOW}[WARNING]${NC} $*"
    fi
    log "WARNING: $*"
}

print_error() {
    echo -e "${RED}[ERROR]${NC} $*"
    log "ERROR: $*"
}

print_quiet() {
    # Always show these messages even in quiet mode, but not in silent mode
    if [[ "$SILENT_MODE" == "false" ]]; then
        echo -e "$*"
    fi
    log "DISPLAY: $*"
}

print_essential() {
    # Only show the most essential output
    # In silent mode, only show critical errors and final results
    if [[ "$SILENT_MODE" != "true" ]] || [[ "$*" == *"SUCCESS"* ]] || [[ "$*" == *"ERROR"* ]] || [[ "$*" == *"PROCESS STOPPED"* ]]; then
        echo -e "$*"
        log "ESSENTIAL: $*"
    fi
}

print_progress() {
    # Progress indicator that overwrites previous line
    printf "\r${BLUE}[PROGRESS]${NC} %s" "$*"
}

print_header() {
    if [[ "$QUIET_MODE" == "false" ]] && [[ "$SILENT_MODE" == "false" ]]; then
        print_quiet "${BLUE}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—${NC}"
        print_quiet "${BLUE}â•‘${NC}           Simple Chrome Extension Tab Opener            ${BLUE}â•‘${NC}"
        print_quiet "${BLUE}â•‘${NC}        No automation - just opens tabs naturally        ${BLUE}â•‘${NC}"
        print_quiet "${BLUE}â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
        echo
    fi
}

# ============================================================================
# UTILITY FUNCTIONS
# ============================================================================

setup_environment() {
    mkdir -p logs
    mkdir -p temp
    
    # Clean up old filtered CSV files (keep only last 3)
    local filtered_files=($(ls -t *filtered*.csv 2>/dev/null))
    if [[ ${#filtered_files[@]} -gt 3 ]]; then
        local files_to_delete=("${filtered_files[@]:3}")
        print_info "Cleaning up ${#files_to_delete[@]} old filtered CSV files..."
        for file in "${files_to_delete[@]}"; do
            [[ -f "$file" ]] && rm "$file" && print_info "Removed: $file"
        done
    fi
    
    # Initialize log file
    echo "Simple Chrome Extension Tab Opener - $(date)" > "$LOG_FILE"
    echo "=========================================" >> "$LOG_FILE"
}

check_dependencies() {
    print_info "Checking dependencies..."
    
    # Check if xdg-open or similar is available for opening URLs
    if ! command -v xdg-open >/dev/null 2>&1; then
        if ! command -v open >/dev/null 2>&1; then  # macOS
            print_error "No URL opener found (xdg-open or open command)"
            return 1
        fi
    fi
    
    # Check backend
    if ! curl -s -f "http://localhost:$BACKEND_PORT/health" >/dev/null 2>&1; then
        print_warning "Backend service not running on port $BACKEND_PORT"
        print_info "Please start the backend service first"
        return 1
    fi
    
    # Check Chrome Extension status
    local recent_extractions
    recent_extractions=$(curl -s "http://localhost:$BACKEND_PORT/api/agents?limit=1" 2>/dev/null)
    
    if [[ $? -eq 0 ]] && echo "$recent_extractions" | grep -q '"agents"'; then
        print_info "Backend accessible"
    else
        print_warning "No recent extractions - Chrome Extension might not be working"
    fi
    
    print_info "Dependencies ready"
    return 0
}

# ============================================================================
# CHROME EXTENSION VERIFICATION
# ============================================================================

verify_chrome_extension() {
    if [[ "$SILENT_MODE" != "true" ]]; then
        print_essential "ğŸ” Verifying Chrome Extension status..."
    fi
    
    # Test extension connectivity by sending a test request
    local test_response
    test_response=$(curl -s -X POST \
        -H "Content-Type: application/json" \
        -d '{"url":"https://test.realtor.com","pageType":"test","agentData":{"name":"Test"}}' \
        "http://localhost:$BACKEND_PORT/api/extract" 2>/dev/null)
    
    if [[ $? -eq 0 ]] && echo "$test_response" | grep -q '"success":false'; then
        print_info "âœ… Backend extract endpoint responding"
    else
        print_error "âŒ Backend extract endpoint not responding properly"
        return 1
    fi
    
    # Check for recent Chrome activity (browser processes)
    local chrome_count
    chrome_count=$(ps aux | grep -c '[c]hrome' || echo "0")
    
    if [[ $chrome_count -gt 0 ]]; then
        print_info "âœ… Chrome browser processes detected ($chrome_count processes)"
    else
        print_warning "âš ï¸ No Chrome processes detected - extension may not be available"
    fi
    
    # Check if extraction logs show recent activity
    local recent_logs
    recent_logs=$(curl -s "http://localhost:$BACKEND_PORT/api/stats" 2>/dev/null)
    
    if [[ $? -eq 0 ]] && echo "$recent_logs" | grep -q '"agents"'; then
        print_info "âœ… Backend statistics accessible"
    else
        print_warning "âš ï¸ Backend statistics not accessible"
    fi
    
    # Only show these messages in non-silent mode
    if [[ "$SILENT_MODE" != "true" ]]; then
        print_essential "ğŸ”§ Extension verification complete"
        print_essential ""
        print_essential "ğŸ“‹ IMPORTANT: Please ensure Chrome Extension is:"
        print_essential "   1. Installed and enabled in Chrome"
        print_essential "   2. Permissions granted for localhost"
        print_essential "   3. Content script active on realtor.com pages"
        print_essential ""
        print_essential "Press Enter to continue, or Ctrl+C to abort..."
        read -r
    else
        # Brief verification message for silent mode - only once
        print_quiet "âœ… Extension verification completed"
    fi
    
    return 0
}

# ============================================================================
# URL MANAGEMENT
# ============================================================================

load_urls_from_csv() {
    local csv_file="$1"
    
    if [[ ! -f "$csv_file" ]]; then
        print_error "CSV file not found: $csv_file"
        return 1
    fi
    
    print_info "Loading URLs from $csv_file..."
    
    # Read URLs, handling different CSV formats
    local urls=()
    local line_count=0
    
    while IFS= read -r line; do
        ((line_count++))
        
        # Skip empty lines
        [[ -z "$line" ]] && continue
        
        # Extract URL (first column or the line itself)
        local url
        if [[ "$line" == *","* ]]; then
            # CSV with multiple columns - take first column
            url=$(echo "$line" | cut -d',' -f1 | tr -d '"' | tr -d "'" | xargs)
        else
            # Single column
            url=$(echo "$line" | xargs)
        fi
        
        # Skip header row (first line that doesn't look like a realtor.com URL)
        if [[ $line_count -eq 1 ]] && [[ ! "$url" =~ ^https?://.*realtor\.com.* ]]; then
            print_info "Skipping header row: $line"
            continue
        fi
        
        # Validate URL
        if [[ "$url" =~ ^https?://.*realtor\.com.* ]]; then
            urls+=("$url")
            print_info "Added URL: $url"
        else
            print_warning "Skipping invalid URL: $url"
        fi
    done < "$csv_file"
    
    TOTAL_URLS=${#urls[@]}
    print_info "Loaded $TOTAL_URLS valid Realtor.com URLs"
    
    # Export URLs array for use in other functions
    printf '%s\n' "${urls[@]}" > temp/urls_to_process.txt
    
    return 0
}

check_url_exists() {
    local url="$1"
    
    local escaped_url
    escaped_url=$(printf '%s' "$url" | sed 's/"/\\"/g' | sed 's/\\/\\\\/g')
    
    local response
    response=$(curl -s -X POST \
        -H "Content-Type: application/json" \
        -d "{\"url\":\"$escaped_url\"}" \
        "http://localhost:$BACKEND_PORT/api/check-duplicate" 2>/dev/null)
    
    if [[ $? -eq 0 ]] && echo "$response" | grep -q '"isDuplicate":true'; then
        return 0  # URL exists
    else
        return 1  # URL doesn't exist
    fi
}

# ============================================================================
# TAB OPENING FUNCTIONS
# ============================================================================

open_url_in_new_tab() {
    local url="$1"
    
    # Add auto-extraction parameter to trigger Chrome Extension automatically
    local auto_url
    if [[ "$url" == *"?"* ]]; then
        auto_url="${url}&autoExtract=true"
    else
        auto_url="${url}?autoExtract=true"
    fi
    
    print_essential "Processing URL ${PROCESSED_URLS}/${TOTAL_URLS}: $(basename "$url")"
    print_info "Opening: $(basename "$url")"
    if [[ "$SILENT_MODE" != "true" ]]; then
        print_info "Auto-extraction URL: $auto_url"
    fi
    
    # Open URL with auto-extraction parameter in default browser (new tab)
    if command -v xdg-open >/dev/null 2>&1; then
        xdg-open "$auto_url" >/dev/null 2>&1 &
    elif command -v open >/dev/null 2>&1; then  # macOS
        open "$auto_url" >/dev/null 2>&1 &
    else
        print_error "Cannot open URL - no suitable command found"
        return 1
    fi
    
    return 0
}

# Play notification sound for successful scrapes
play_notification() {
    local notification_type="${1:-scrape}"  # Default to 'scrape', can be 'completion'
    local sound_file
    
    if [[ "$notification_type" == "completion" ]]; then
        sound_file="../crm-app/public/sting_econsuc2.mp3"
        print_quiet "${GREEN}ğŸ‰ Playing completion notification...${NC}"
    else
        sound_file="../crm-app/public/Notification.mp3"
        print_quiet "${BLUE}ğŸ”” Playing scrape notification...${NC}"
    fi
    
    # Check if the sound file exists
    if [[ ! -f "$sound_file" ]]; then
        print_quiet "${YELLOW}âš ï¸ Audio file not found: $sound_file${NC}"
        printf '\a\a'  # Fallback beep
        return
    fi
    
    # Try multiple audio players
    if command -v paplay >/dev/null 2>&1; then
        paplay "$sound_file" >/dev/null 2>&1 &
    elif command -v aplay >/dev/null 2>&1; then
        aplay "$sound_file" >/dev/null 2>&1 &
    elif command -v mpg123 >/dev/null 2>&1; then
        mpg123 -q "$sound_file" >/dev/null 2>&1 &
    elif command -v ffplay >/dev/null 2>&1; then
        ffplay -nodisp -autoexit "$sound_file" >/dev/null 2>&1 &
    elif command -v cvlc >/dev/null 2>&1; then
        cvlc --play-and-exit "$sound_file" >/dev/null 2>&1 &
    else
        # Enhanced fallback: multiple beeps
        printf '\a\a\a'
        sleep 0.5
        printf '\a\a'
    fi
}

# Play completion sound for finished list
play_completion_sound() {
    # Path to the completion sound
    local sound_file="../crm-app/public/sting_econsuc2.mp3"
    
    # Check if the sound file exists
    if [[ ! -f "$sound_file" ]]; then
        print_quiet "${YELLOW}âš ï¸ Completion audio file not found: $sound_file${NC}"
        printf '\a\a\a\a\a'  # Fallback beeps
        return
    fi
    
    print_quiet "${GREEN}ğŸ‰ Playing completion celebration sound...${NC}"
    
    # Try multiple audio players
    if command -v paplay >/dev/null 2>&1; then
        paplay "$sound_file" >/dev/null 2>&1 &
    elif command -v aplay >/dev/null 2>&1; then
        aplay "$sound_file" >/dev/null 2>&1 &
    elif command -v mpg123 >/dev/null 2>&1; then
        mpg123 -q "$sound_file" >/dev/null 2>&1 &
    elif command -v ffplay >/dev/null 2>&1; then
        ffplay -nodisp -autoexit "$sound_file" >/dev/null 2>&1 &
    elif command -v cvlc >/dev/null 2>&1; then
        cvlc --play-and-exit "$sound_file" >/dev/null 2>&1 &
    else
        # Enhanced fallback: celebration beeps
        printf '\a\a\a\a\a'
        sleep 0.3
        printf '\a\a\a'
        sleep 0.3
        printf '\a\a'
    fi
}

# Show current progress summary
show_progress_summary() {
    local percent=$((PROCESSED_URLS * 100 / TOTAL_URLS))
    echo
    print_quiet "${GREEN}â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”${NC}"
    print_quiet "${GREEN}â”‚                    PROGRESS SUMMARY                    â”‚${NC}"
    print_quiet "${GREEN}â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤${NC}"
    print_quiet "${GREEN}â”‚${NC} Progress: ${percent}% (${PROCESSED_URLS}/${TOTAL_URLS}) ${GREEN}â”‚${NC}"
    print_quiet "${GREEN}â”‚${NC} Successful: ${SUCCESSFUL_EXTRACTIONS} | Failed: ${FAILED_URLS} | Skipped: ${SKIPPED_URLS} ${GREEN}â”‚${NC}"
    if [[ -n "$LAST_REALTOR_NAME" ]]; then
        print_quiet "${GREEN}â”‚${NC} Last Realtor: ${LAST_REALTOR_NAME:0:40} ${GREEN}â”‚${NC}"
    fi
    print_quiet "${GREEN}â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜${NC}"
    echo
}

# Extract realtor name from API response for display
extract_realtor_name() {
    local url="$1"
    local escaped_url
    escaped_url=$(printf '%s' "$url" | sed 's/"/\\"/g' | sed 's/\\/\\\\/g')
    
    # Try to get realtor data from backend
    local response
    response=$(curl -s -X POST \
        -H "Content-Type: application/json" \
        -d "{\"url\":\"$escaped_url\"}" \
        "http://localhost:$BACKEND_PORT/api/check-duplicate" 2>/dev/null)
    
    if [[ $? -eq 0 ]] && echo "$response" | grep -q '"success":true'; then
        # Extract name from response if available
        local name
        name=$(echo "$response" | grep -o '"name":"[^"]*"' | sed 's/"name":"//; s/"//')
        if [[ -n "$name" ]]; then
            LAST_REALTOR_NAME="$name"
            LAST_REALTOR_URL="$url"
        fi
    fi
}

# Poll for URL completion with real-time feedback
poll_for_completion() {
    local url="$1"
    local max_wait="${PROCESSING_WAIT}"
    local elapsed=0
    local check_count=0
    
    # Check if this URL was already successfully processed in this session
    if [[ -n "${SUCCESSFULLY_PROCESSED_URLS[$url]:-}" ]]; then
        print_essential "â­ï¸  Already processed in this session: $(basename "$url")"
        return 0
    fi
    
    # Quiet mode - just show basic progress
    print_progress "Processing $(basename "$url") - waiting for Chrome Extension..."
    
    while [[ $elapsed -lt $max_wait ]]; do
        ((check_count++))
        
        # Check if URL now exists in database
        local escaped_url
        escaped_url=$(printf '%s' "$url" | sed 's/"/\\"/g' | sed 's/\\/\\\\/g')
        
        local response
        response=$(curl -s -X POST \
            -H "Content-Type: application/json" \
            -d "{\"url\":\"$escaped_url\"}" \
            "http://localhost:$BACKEND_PORT/api/check-duplicate" 2>/dev/null)
        
        local curl_exit_code=$?
        
        if [[ $curl_exit_code -eq 0 ]]; then
            if echo "$response" | grep -q '"success":true'; then
                if echo "$response" | grep -q '"isDuplicate":true'; then
                    # For force reprocessing, we need to verify the record was actually updated
                    if [[ "$FORCE_REPROCESS" == "true" ]]; then
                        # Extract last scraped timestamp from the response
                        local last_scraped=$(echo "$response" | grep -o '"lastScraped":"[^"]*"' | cut -d'"' -f4)
                        
                        if [[ -n "$last_scraped" ]]; then
                            # Check if the timestamp is very recent (within last 30 seconds)
                            local current_time=$(date +%s)
                            local scraped_time=$(date -d "$last_scraped" +%s 2>/dev/null || echo "0")
                            local time_diff=$((current_time - scraped_time))
                            
                            if [[ $time_diff -lt 30 ]]; then
                                # Mark this URL as successfully processed in this session
                                SUCCESSFULLY_PROCESSED_URLS[$url]=1
                                echo # Clear progress line
                                extract_realtor_name "$url"
                                print_essential "âœ… SUCCESS! Data re-extracted (${elapsed}s) - ${LAST_REALTOR_NAME:-"Unknown Agent"}"
                                play_notification
                                return 0
                            else
                                print_info "Force reprocess: Last scraped ${time_diff}s ago, waiting for fresh update..."
                            fi
                        else
                            print_info "Force reprocess: No last scraped timestamp found, continuing to wait..."
                        fi
                    else
                        # Normal mode - duplicate found means already processed
                        # Mark this URL as successfully processed in this session
                        SUCCESSFULLY_PROCESSED_URLS[$url]=1
                        echo # Clear progress line
                        extract_realtor_name "$url"
                        print_essential "âœ… SUCCESS! Data extracted (${elapsed}s) - ${LAST_REALTOR_NAME:-"Unknown Agent"}"
                        play_notification
                        return 0
                    fi
                fi
            fi
        fi
        
        # Show minimal progress (only every 30 seconds to reduce log size)
        if (( elapsed % 30 == 0 )); then
            local remaining=$((max_wait - elapsed))
            print_progress "Processing $(basename "$url") - ${elapsed}s elapsed, ${remaining}s remaining..."
        fi
        
        sleep "$POLLING_INTERVAL"
        ((elapsed += POLLING_INTERVAL))
    done
    
    echo # Clear progress line
    
    # Before giving up, do one final check
    local escaped_url
    escaped_url=$(printf '%s' "$url" | sed 's/"/\\"/g' | sed 's/\\/\\\\/g')
    
    local final_response
    final_response=$(curl -s -X POST \
        -H "Content-Type: application/json" \
        -d "{\"url\":\"$escaped_url\"}" \
        "http://localhost:$BACKEND_PORT/api/check-duplicate" 2>/dev/null)
    
    if [[ $? -eq 0 ]] && echo "$final_response" | grep -q '"isDuplicate":true'; then
        SUCCESSFULLY_PROCESSED_URLS[$url]=1
        extract_realtor_name "$url"
        print_essential "âœ… SUCCESS! Data found after timeout - ${LAST_REALTOR_NAME:-"Unknown Agent"}"
        play_notification
        return 0
    fi
    
    # If we get here, something is wrong - but mark as processed to avoid infinite loop
    SUCCESSFULLY_PROCESSED_URLS[$url]=1
    print_warning "âš ï¸ Chrome Extension timeout - but AUTO-CONTINUING anyway"
    print_warning "ğŸ“‹ AUTO-ASSUME: Assuming extension worked (no manual verification)"
    print_info "ğŸ“‹ AUTOMATIC MODE: Treating URL as successfully processed"
    print_info "   âœ… URL marked as processed automatically"
    print_info "   ğŸ”„ Continuing to next URL..."
    
    print_success "âœ… URL marked as auto-processed (timeout override)"
    play_notification
    return 0
}

process_url_batch() {
    local batch_urls=("$@")
    
    for url in "${batch_urls[@]}"; do
        # Check if URL already exists (unless force reprocess is enabled)
        if [[ "$FORCE_REPROCESS" == "false" ]] && check_url_exists "$url"; then
            ((PROCESSED_URLS++))
            print_warning "â­ï¸  Already exists: $(basename "$url")"
            ((SKIPPED_URLS++))
            continue
        elif [[ "$FORCE_REPROCESS" == "true" ]]; then
            print_info "ğŸ”„ Force reprocessing: $(basename "$url")"
        fi
        
        # Increment counter BEFORE processing for correct display
        ((PROCESSED_URLS++))
        
        # Open URL in new tab
        if open_url_in_new_tab "$url"; then
            # Poll for completion
            if poll_for_completion "$url"; then
                ((SUCCESSFUL_EXTRACTIONS++))
            else
                print_warning "âš ï¸ Timeout (auto-continuing): $(basename "$url")"
                ((SUCCESSFUL_EXTRACTIONS++))  # Count as success per user requirement
            fi
            
            # Small delay before next URL
            if [[ ${#batch_urls[@]} -gt 1 ]]; then
                sleep "$TAB_DELAY"
            fi
        else
            print_error "âŒ Failed to open: $(basename "$url")"
            ((FAILED_URLS++))
        fi
    done
    
    return 0
}

# ============================================================================
# PROGRESS TRACKING
# ============================================================================

save_progress() {
    local progress_data
    progress_data=$(cat << EOF
{
    "timestamp": "$(date -Iseconds)",
    "total_urls": $TOTAL_URLS,
    "processed": $PROCESSED_URLS,
    "successful": $SUCCESSFUL_EXTRACTIONS,
    "skipped": $SKIPPED_URLS,
    "failed": $FAILED_URLS,
    "progress_percent": $(( PROCESSED_URLS * 100 / TOTAL_URLS )),
    "processing_mode": "single_url",
    "url_delay": $TAB_DELAY,
    "polling_interval": $POLLING_INTERVAL,
    "max_wait": $PROCESSING_WAIT
}
EOF
)
    
    echo "$progress_data" > "$PROGRESS_FILE"
}

print_progress() {
    local percent=$((PROCESSED_URLS * 100 / TOTAL_URLS))
    
    print_info "Progress: $PROCESSED_URLS/$TOTAL_URLS ($percent%)"
    print_info "Success: $SUCCESSFUL_EXTRACTIONS, Skipped: $SKIPPED_URLS, Failed: $FAILED_URLS"
    
    if [[ $PROCESSED_URLS -lt $TOTAL_URLS ]]; then
        local remaining=$((TOTAL_URLS - PROCESSED_URLS))
        # For single URL processing: remaining URLs * (average processing time per URL)
        # Assume average of 15 seconds per URL (polling + processing time)
        local est_minutes=$(( remaining * 15 / 60 ))
        print_info "Estimated time remaining: $est_minutes minutes (single-URL processing)"
    fi
}

print_final_summary() {
    print_success "Processing completed!"
    echo
    print_info "Final Statistics:"
    print_info "  Total URLs: $TOTAL_URLS"
    print_info "  Processed: $PROCESSED_URLS"
    print_info "  Successful extractions: $SUCCESSFUL_EXTRACTIONS"
    print_info "  Skipped (already existed): $SKIPPED_URLS"
    print_info "  Failed: $FAILED_URLS"
    
    if [[ $PROCESSED_URLS -gt 0 ]]; then
        local success_rate=$((SUCCESSFUL_EXTRACTIONS * 100 / PROCESSED_URLS))
        print_info "  Success rate: $success_rate%"
    fi
    
    print_info ""
    print_info "Files created:"
    print_info "  Log file: $LOG_FILE"
    print_info "  Progress file: $PROGRESS_FILE"
}

# ============================================================================
# MAIN PROCESSING
# ============================================================================

process_all_urls() {
    local csv_file="$1"
    
    # Load URLs
    if ! load_urls_from_csv "$csv_file"; then
        return 1
    fi
    
    if [[ $TOTAL_URLS -eq 0 ]]; then
        print_error "No valid URLs found to process"
        return 1
    fi
    
    if [[ "$QUIET_MODE" == "false" ]]; then
        print_quiet "Starting batch processing of ${TOTAL_URLS} URLs"
        print_quiet "Processing: Single URL at a time, ${TAB_DELAY}s delay between URLs"
        echo
    fi
    
    # Read URLs into array
    mapfile -t all_urls < temp/urls_to_process.txt
    
    # Process in batches
    for ((i=0; i<TOTAL_URLS; i+=BATCH_SIZE)); do
        # Check for stop signal immediately
        if ! kill -0 $$ 2>/dev/null; then
            print_essential "Process termination detected, stopping..."
            break
        fi
        
        local batch_end=$((i + BATCH_SIZE))
        if [[ $batch_end -gt $TOTAL_URLS ]]; then
            batch_end=$TOTAL_URLS
        fi
        
        local batch_urls=("${all_urls[@]:$i:$((batch_end - i))}")
        local batch_num=$(( (i / BATCH_SIZE) + 1 ))
        local total_batches=$(( (TOTAL_URLS + BATCH_SIZE - 1) / BATCH_SIZE ))
        
        process_url_batch "${batch_urls[@]}"
        
        # Save progress and show summary every 5 URLs
        save_progress
        if [[ $((PROCESSED_URLS % 5)) -eq 0 ]] || [[ $PROCESSED_URLS -eq $TOTAL_URLS ]]; then
            show_progress_summary
        fi
        echo
        
        # Small delay before next batch - check for stop signal during delay
        if [[ $batch_end -lt $TOTAL_URLS ]]; then
            sleep 1 &
            wait $! 2>/dev/null || break  # If sleep is interrupted, stop processing
        fi
    done
    
    print_final_summary
}

# Show final processing summary
print_final_summary() {
    echo
    print_quiet "${GREEN}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—${NC}"
    print_quiet "${GREEN}â•‘                    BATCH PROCESSING COMPLETE             â•‘${NC}"
    print_quiet "${GREEN}â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£${NC}"
    print_quiet "${GREEN}â•‘${NC} Total URLs: ${TOTAL_URLS} ${GREEN}â•‘${NC}"
    print_quiet "${GREEN}â•‘${NC} Processed: ${PROCESSED_URLS} ${GREEN}â•‘${NC}"
    print_quiet "${GREEN}â•‘${NC} Successful: ${SUCCESSFUL_EXTRACTIONS} ${GREEN}â•‘${NC}"
    print_quiet "${GREEN}â•‘${NC} Skipped: ${SKIPPED_URLS} ${GREEN}â•‘${NC}"
    print_quiet "${GREEN}â•‘${NC} Failed: ${FAILED_URLS} ${GREEN}â•‘${NC}"
    if [[ -n "$LAST_REALTOR_NAME" ]]; then
        print_quiet "${GREEN}â•‘${NC} Last Realtor: ${LAST_REALTOR_NAME:0:35} ${GREEN}â•‘${NC}"
    fi
    print_quiet "${GREEN}â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
    echo
    
    # Play completion sound
    play_completion_sound
}

# ============================================================================
# SIGNAL HANDLING AND CLEANUP
# ============================================================================

cleanup() {
    print_info "Cleaning up..."
    rm -f temp/urls_to_process.txt
}

trap cleanup EXIT
trap 'print_error "Interrupted by user"; exit 130' INT TERM

# ============================================================================
# COMMAND LINE INTERFACE
# ============================================================================

show_usage() {
    cat << EOF
Simple Chrome Extension Tab Opener

Processes URLs from CSV file one at a time with real-time completion detection.
No browser automation - just opens tabs naturally.

Usage: $0 [OPTIONS] [CSV_FILE]

OPTIONS:
    -h, --help          Show this help message
    -d, --delay SEC     Seconds between processing each URL (default: $TAB_DELAY)

ARGUMENTS:
    CSV_FILE           Path to CSV file with URLs (default: $DEFAULT_CSV_FILE)

EXAMPLES:
    $0                                    # Process default CSV file
    $0 my_urls.csv                       # Process custom CSV file
    $0 -d 3 urls.csv                     # 3 second delay between URLs

REQUIREMENTS:
    - Chrome Extension must be loaded in browser
    - Backend service running on port $BACKEND_PORT
    - Chrome/Firefox browser as default browser

HOW IT WORKS:
    1. Opens URLs one at a time using system default browser
    2. Chrome Extension automatically detects and processes pages
    3. Polls backend every ${POLLING_INTERVAL}s to detect completion
    4. Plays notification sound when each URL completes
    5. Automatically moves to next URL after completion/timeout
    6. Provides real-time progress tracking and statistics

EOF
}

# ============================================================================
# TEST MODE FUNCTION
# ============================================================================

process_single_url_test() {
    local csv_file="$1"
    
    # Load URLs
    if ! load_urls_from_csv "$csv_file"; then
        return 1
    fi
    
    if [[ $TOTAL_URLS -eq 0 ]]; then
        print_error "No valid URLs found to process"
        return 1
    fi
    
    # Read first URL only
    local test_url=$(head -1 temp/urls_to_process.txt)
    
    print_info "ğŸ§ª Testing with first URL: $test_url"
    print_info "This will test the Chrome Extension integration and polling system"
    echo
    
    # Process just the first URL
    print_info "Opening test URL in browser..."
    if ! open_url_in_new_tab "$test_url"; then
        print_error "Failed to open URL: $test_url"
        return 1
    fi
    
    print_info "ğŸ”„ Polling for completion (Chrome Extension should save data)..."
    if poll_for_completion "$test_url"; then
        print_success "âœ… Test successful! Chrome Extension processed the URL"
        play_notification
        return 0
    else
        print_warning "âš ï¸ Test timed out - Chrome Extension may not be working"
        print_info "AUTO-ASSUME: Assuming Chrome Extension is working (no manual check)"
        print_warning "ğŸš€ AUTO-CONTINUE: Proceeding without manual confirmation..."
        print_success "âœ… Auto-confirmation: Test considered successful"
        play_notification
        return 0
    fi
}

main() {
    local csv_file="$DEFAULT_CSV_FILE"
    
    # If running from API (has certain args), enable ultra-quiet mode
    for arg in "$@"; do
        if [[ "$arg" == *".csv" ]]; then
            SILENT_MODE=true
            break
        fi
    done
    
    # Parse command line arguments
    while [[ $# -gt 0 ]]; do
        case $1 in
            -h|--help)
                show_usage
                exit 0
                ;;
            -d|--delay)
                TAB_DELAY="$2"
                shift 2
                ;;
            -f|--force)
                FORCE_REPROCESS=true
                shift
                ;;
            -*)
                print_error "Unknown option: $1"
                show_usage
                exit 1
                ;;
            *)
                csv_file="$1"
                shift
                ;;
        esac
    done
    
    # Display header
    print_header
    
    # Setup
    setup_environment
    
    # Check dependencies
    if ! check_dependencies; then
        exit 1
    fi
    
    # Verify Chrome Extension is working
    if ! verify_chrome_extension; then
        print_error "Chrome Extension verification failed"
        exit 1
    fi
    
    # Only show essential config in silent mode
    if [[ "$SILENT_MODE" == "false" ]]; then
        print_info "Configuration:"
        print_info "  CSV File: $csv_file"
        print_info "  Processing Mode: Single URL at a time"
        print_info "  Delay Between URLs: ${TAB_DELAY}s"
        echo
    fi
    
    # Process all URLs automatically (no test mode)
    if process_all_urls "$csv_file"; then
        print_essential "All processing completed successfully!"
        exit 0
    else
        print_error "Processing failed"
        exit 1
    fi
}

# Run main function
main "$@"
